# Parameters to setup experiment.
experiment:
  # Experiment logs and checkpoints will be stored in:
  logdir: projects/VolumetricEnhance/logs/new_scene_planes
  # Seed for random number generators (for repeatability).
  randomseed: 0 # 34  # 8239
  # Number of training iterations.
  train_iters: 750000
  # Automatically stop training if no improvement in the relevant loss (the term used for determining best checkpoint so far) is observed for this many iterations:
  no_improvement_iters: 25000 # Number of iteration PER-TRAINING SCENE
  # Evaluation round frequency:
  validate_every: [0.1,5000] # [Desired portion of evaluation time, compared to training time,Max training iterations between evaluation rounds]
  # Checkpoint saving frequency:
  save_every: 10.0 # Minutes (Use floating point numbers)
  # Number of training iterations after which to print progress:
  print_every: 100

# Dataset parameters.
dataset:
  # Per dataset type configurations:
  synt: # Synthetic scenes, from the NeRF dataset or using the same configuration
    root: datasets/Synthetic
    # Near clip plane (clip all depth values closer than this threshold).
    near: 2
    # Far clip plane (clip all depth values farther than this threshold).
    far:  6
    # Do not use NDC (normalized device coordinates). Usually True for
    # synthetic (Blender) datasets.
    no_ndc: True
  llff: # Real world scenes from NeRF:
    root: datasets/LLFF
    # Near clip plane (clip all depth values closer than this threshold).
    near: 0
    # Far clip plane (clip all depth values farther than this threshold).
    far:  1
    # Do not use NDC (normalized device coordinates). Usually True for
    # synthetic (Blender) datasets.
    no_ndc: False
    # If set, adding test views by interpolating camera poses, to allow rendering smoother high FPS video sequences:
    # min_eval_frames:  200
  max_scenes_eval:  2 # If set, using at most this number of evaluation scenes in evaluation rounds:
  # prob_assigned2scene_groups: False # (Default: True) When True, the probability value reflects the chance of sampling A (any) SCENE IN THE GROUP, and not the probabiliy of sampling any specific scene.
  dir:
    train:
      # Training scenes: 
      # Parameter convention:
      #   downsampling_factor,pos_feature_plane_resolution,viewdir_feature_plane_resoution,dataset_type (default:'synt'),relative probability (default:1): [scene names]
      # Adding suffix "##<integer>" to scene name allows training multiple independent feature plane sets to the same scene. E.g. ['lego','ship','lego##1','lego##2']
      # LR scenes (training and some of evaluation):
      8,200,32:   ['mic##2','ship##2','chair##2','lego##2',]
    val:
      # Evaluation scenes (Should not overlap with training scenes):
      # Use the same parameter convention as in training scenes, without sampling probability
      2,800,32:   ['mic##2','ship##2','chair##2','lego##2',] #'ficus##1','materials##1','hotdog##1','drums##1'
  testskip: 10 # Stride for synthetic scenes: Include one evaluation image per "testskip" images in the dataset.
  llffhold: 2 # For real scenes: Number of training images to hold out for evaluation

# Model parameters.
models:
  # Path to pre-trained model checkpoint to resume (if set):
  path: projects/VolumetricEnhance/logs/E2E_Synt_Res25Sc200_21Sc800_32_LR100_400_noDetach_Lean_other4NoPaper4_0_copy1459K

# Optimizer params.
optimizer:
  # Name of the torch.optim class used for optimization.
  type: Adam
  # Learning rate.
  planes_lr: 4.E-3

# NeRF parameters.
nerf:
  # Use viewing directions as input, in addition to the X, Y, Z coordinates.
  use_viewdirs: True
  # Normalize elevation and azimuth angle ranges to the range observed across training image frames
  # adjust_elevation_range: 1 # Default: False
  # adjust_azimuth_range: True # Default: False
  train: # Training-specific parameters:
    # Which modules to train:
    what: ['LR_planes',]
    # Number of random rays to retain from each image. These sampled rays are used for training, while the rest are discarded:
    num_random_rays: 4096 #3072
    # Size of each chunk (rays are batched into "chunks" and passed through the network):
    chunksize: 131072
    # Store and load plane parameters using hard drive to facilitate simultaneously training on many scenes with limited GPU memory:
    store_planes:
      # buffer_size:  10 # Generally there is not much sense in using a buffer size here, since training of feature planes for different scenes is independent, so an alternative would be to use multiple training runs with fewer scenes each, to meet the memory limitation.
      steps_per_buffer: 5 # Number of training iterations before re-drawing a new set of scenes
    # Whether or not to perturb the sampled depth values along each ray:
    perturb: True
    # Feature planes weights initialization factor, relative to STD of a weights in a specific layer of the pre-trained decoder model:
    STD_factor: 0.34
    # Number of depth samples per ray for the coarse network:
    num_coarse: 64
    # Number of depth samples per ray for the fine network:
    num_fine: 64
    # Whether to render models using a white background:
    white_background: False
    # Standard deviation of noise to be added to the radiance field when performing volume rendering.
    radiance_field_noise_std: 0.2
    # Sample linearly in disparity space, as opposed to in depth space.
    lindisp: False
  validation:   # Validation-specific parameters:
    chunksize: 131072
    # Whether or not to perturb the sampled depth values:
    perturb: False
    # Number of depth samples per ray for the coarse network:
    num_coarse: 64
    # Number of depth samples per ray for the fine network:
    num_fine: 64
    # Whether to render models using a white background:
    white_background: False
    # Standard deviation of noise to be added to the radiance field when performing volume rendering:
    radiance_field_noise_std: 0.
    # Sample linearly in disparity space, as opposed to in depth space.
    lindisp: False