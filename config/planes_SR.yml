# Parameters to setup experiment.
experiment:
  # Unique experiment identifier
  id: SR6vs2Scenes_32B256C_LR4_NoSpatSamp_planeNorm_consLoss_01 #SR2vs1Scenes_16B256C_SFpos # SR_dirs_encoding_L4C256W0001  # SR_xyz_encoding # debug-lego200
  # Experiment logs will be stored at "logdir"/"id"
  logdir: /scratch/gpfs/yb6751/projects/VolumetricEnhance/logs
  # Seed for random number generators (for repeatability).
  randomseed: 0 # 34  # 8239
  # Number of training iterations.
  train_iters: 750000
  # Number of training iterations after which to validate.
  validate_every: [0.1,1000] #50
  # Number of training iterations after which to checkpoint.
  save_every: 10.0 #5000
  # Number of training iterations after which to print progress.
  print_every: 100

# Dataset parameters.
dataset:
  # Base directory of dataset.
  root: /scratch/gpfs/yb6751/projects/VolumetricEnhance/cache/nerf_synthetic
  dir:
    train: ['chair','drums','ficus','hotdog','lego','materials',]
    # train: ['chair'] #,'drums','ficus','hotdog','lego','materials','mic','ship']
    # train: ['chair','drums'] #,'ficus','hotdog','lego','materials',]
    # train: ['ficus','hotdog'] #,'lego','materials',]
    # train: lego
    val: ['mic','ship']
    # val: mic
  # Optionally, provide a path to the pre-cached dataset dir. This
  # overrides the other dataset options.
  cachedir: cache/legocache200
  # For the Blender datasets (synthetic), optionally return images
  # at half the original resolution of 800 x 800, to save space.
  half_res: True
  # Stride (include one per "testskip" images in the dataset).
  testskip: 10
  # Do not use NDC (normalized device coordinates). Usually True for
  # synthetic (Blender) datasets.
  no_ndc: True
  # Near clip plane (clip all depth values closer than this threshold).
  near: 2
  # Far clip plane (clip all depth values farther than this threshold).
  far: 6

# Model parameters.
models:
  path: /scratch/gpfs/yb6751/projects/VolumetricEnhance/logs/planes_Res200_32_DSpos12_Cdec8skip3CplnCat_LR100_allScenes_0

# Optimizer params.
optimizer:
  # Name of the torch.optim class used for optimization.
  type: Adam
  # Learning rate.
  lr: 5.0E-5 #1.0E-4 # 5.0E-3

# NeRF parameters.
nerf:
  # Use viewing directions as input, in addition to the X, Y, Z coordinates.
  use_viewdirs: True
  train:
    # if specified, spatial_sampling is the factor that multiplies the necessary patch area to yield num_random_rays. Should be >=1. Then rays are randomly sampled from this area.
    # spatial_sampling: 1.
    # Number of random rays to retain from each image.
    # These sampled rays are used for training, and the others are discarded.
    num_random_rays: 4096  # 32 * 32 * 4
    # When using spacial patch sampling, how to calculate the sampling distribution: By estimating non-background areas (default) or based on patch-STD
    # spatial_patch_sampling: STD # STD, background_est
    # Simulating a batch by accumulating gradients over consecutive steps:
    # virtual_batch_size: 100
    # Size of each chunk (rays are batched into "chunks" and passed through
    # the network)
    chunksize: 131072  # 131072  # 1024 * 32
    # Save GPU memory by holding plane variables on cpu memory until needed:
    # save_GPU_memory: True
    store_planes:
      # buffer_size:  1 #3 #5
      steps_per_buffer: 1 #40
    # Whether or not to perturb the sampled depth values.
    perturb: True
    # Number of depth samples per ray for the coarse network.
    num_coarse: 64
    # Number of depth samples per ray for the fine network.
    num_fine: 64
    # Whether to render models using a white background.
    white_background: False
    # Standard deviation of noise to be added to the radiance field when
    # performing volume rendering.
    radiance_field_noise_std: 0.2
    # Sample linearly in disparity space, as opposed to in depth space.
    lindisp: False
  # Validation-specific parameters.
  validation:
    # Number of random rays to retain from each image.
    # These sampled rays are used for training, and the others are discarded.
    chunksize: 131072   # 1024 * 32
    # Whether or not to perturb the sampled depth values.
    perturb: False
    # Number of depth samples per ray for the coarse network.
    num_coarse: 64
    # Number of depth samples per ray for the fine network.
    num_fine: 64
    # Whether to render models using a white background.
    white_background: False
    # Standard deviation of noise to be added to the radiance field when
    # performing volume rendering.
    radiance_field_noise_std: 0.
    # Sample linearly in disparity space, as opposed to in depth space.
    lindisp: False

super_resolution:
  # dataset:
    # Use specific HR training images (by passing image indices):
    # train_im_inds: [0] # [0,1,2,3,4,5,6,7,8,9] # 0.1 #
  training:
  #   LR_ims_chance: 0.5
    loss: fine # fine,coarse,both
  apply_2_coarse: False #True # False
  # Super-resolve view-directions plane as well, or only positional planes:
  SR_viewdir: False
  input_normalization:  True
  consistentcy_loss_w: 0.1
  model:
    # Name of the torch.nn.Module class that implements the model.
    type: EDSR
    # Number of layers in the model.
    # Number of hidden units in each layer of the MLP (multi-layer
    # perceptron).
    hidden_size: 256 #128 #256
    # Number of residual blocks (originally 32):
    n_blocks: 32
    # scale_factor: one #one #sqrt #linear
